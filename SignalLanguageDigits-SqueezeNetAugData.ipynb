{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho Final\n",
    "Neste trabalho, vamos buscar o reconhecimento dos dígitos da Linguagem de Sinais. Para tal, vamos nos utilizar de Modelos conhecidos de Deep Learning e também nos aventurar na criação de próprios.\n",
    "\n",
    "------\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from random import sample, seed\n",
    "seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (15,15) # Make the figures a bit bigger\n",
    "\n",
    "# Keras imports\n",
    "from keras.layers import Input, Convolution2D, MaxPooling2D, Activation, concatenate, Dropout, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "\n",
    "import inf619utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "O dataset é composto por 10 classes (dígitos de 0 a 9) com aproximadamente 205 imagens por classe. \n",
    "O conjunto foi dividido em 60% para treinamento, 15% para validação e 20% para teste.\n",
    "As imagens estão divididas em blocos por classe.\n",
    "\n",
    "** IMPORTANTE NÃO ALTERAR O NOME/LOCAL DAS IMAGENS** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetDir = \"./Dataset\"\n",
    "nbClasses = 10\n",
    "input_shape = (100,100,3)\n",
    "\n",
    "train_files_adam = {}\n",
    "val_files_adam = {}\n",
    "test_files_adam = {}\n",
    "\n",
    "train_files_adam, val_files_adam, test_files_adam = inf619utils.splitData(datasetDir, nbClasses)\n",
    "\n",
    "train_files_adadelta = {}\n",
    "val_files_adadelta = {}\n",
    "test_files_adadelta = {}\n",
    "\n",
    "train_files_adadelta, val_files_adadelta, test_files_adadelta = inf619utils.splitData(datasetDir, nbClasses)\n",
    "\n",
    "train_files_sgd = {}\n",
    "val_files_sgd = {}\n",
    "test_files_sgd = {}\n",
    "\n",
    "train_files_sgd, val_files_sgd, test_files_sgd = inf619utils.splitData(datasetDir, nbClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the images from imgList\n",
    "def plotImagesFromBatch(imgList):\n",
    "    for i in range(len(imgList)):\n",
    "        plotImage(imgList[i])\n",
    "\n",
    "\n",
    "# Se quiser visualizar algum bloco de imagens, descomentar as linhas abaixo\n",
    "# inf619utils.plotImages(val_files)\n",
    "# inf619utils.plotImages(train_files)\n",
    "# inf619utils.plotImages(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# images in Train set:  1242\n",
      "# images in Val set:  309\n",
      "# images in Test set:  511\n"
     ]
    }
   ],
   "source": [
    "trainSetSize = inf619utils.getDatasetSize(train_files_adam)\n",
    "valSetSize = inf619utils.getDatasetSize(val_files_adam)\n",
    "testSetSize = inf619utils.getDatasetSize(test_files_adam)\n",
    "\n",
    "print(\"# images in Train set: \", trainSetSize)\n",
    "print(\"# images in Val set: \", valSetSize)\n",
    "print(\"# images in Test set: \", testSetSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição do modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fire Module Definition\n",
    "sq1x1 = \"squeeze1x1\"\n",
    "exp1x1 = \"expand1x1\"\n",
    "exp3x3 = \"expand3x3\"\n",
    "relu = \"relu_\"\n",
    "\n",
    "def fire_module(x, fire_id, squeeze=16, expand=64):\n",
    "    s_id = 'fire' + str(fire_id) + '/'\n",
    "\n",
    "    channel_axis = 3\n",
    "    \n",
    "    x = Convolution2D(squeeze, (1, 1), padding='valid', name=s_id + sq1x1)(x)\n",
    "    x = Activation('relu', name=s_id + relu + sq1x1)(x)\n",
    "\n",
    "    left = Convolution2D(expand, (1, 1), padding='valid', name=s_id + exp1x1)(x)\n",
    "    left = Activation('relu', name=s_id + relu + exp1x1)(left)\n",
    "\n",
    "    right = Convolution2D(expand, (3, 3), padding='same', name=s_id + exp3x3)(x)\n",
    "    right = Activation('relu', name=s_id + relu + exp3x3)(right)\n",
    "\n",
    "    x = concatenate([left, right], axis=channel_axis, name=s_id + 'concat')\n",
    "    return x\n",
    "\n",
    "#SqueezeNet model definition\n",
    "def SqueezeNet(input_shape):\n",
    "    img_input = Input(shape=input_shape) #placeholder\n",
    "    \n",
    "    x = Convolution2D(64, (3, 3), strides=(2, 2), padding='valid', name='conv1')(img_input)\n",
    "    x = Activation('relu', name='relu_conv1')(x)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool1')(x)\n",
    "\n",
    "    x = fire_module(x, fire_id=2, squeeze=16, expand=64)\n",
    "    x = fire_module(x, fire_id=3, squeeze=16, expand=64)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool3')(x)\n",
    "\n",
    "    x = fire_module(x, fire_id=4, squeeze=32, expand=128)\n",
    "    x = fire_module(x, fire_id=5, squeeze=32, expand=128)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool5')(x)\n",
    "\n",
    "    x = fire_module(x, fire_id=6, squeeze=48, expand=192)\n",
    "    x = fire_module(x, fire_id=7, squeeze=48, expand=192)\n",
    "    x = fire_module(x, fire_id=8, squeeze=64, expand=256)\n",
    "    x = fire_module(x, fire_id=9, squeeze=64, expand=256)\n",
    "    \n",
    "    x = Dropout(0.5, name='drop9')(x)\n",
    "\n",
    "    x = Convolution2D(1000, (1, 1), padding='valid', name='conv10')(x)\n",
    "    x = Activation('relu', name='relu_conv10')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Activation('softmax', name='loss')(x)\n",
    "\n",
    "    model = Model(img_input, x, name='squeezenet')\n",
    "\n",
    "    # Download and load ImageNet weights\n",
    "    model.load_weights('./squeezenet_weights_tf_dim_ordering_tf_kernels.h5')\n",
    "    \n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for batch, labels in inf619utils.loadDatasetInBatches(train_files_adam, batch_size=32, input_shape=input_shape, nbClasses=nbClasses, shouldAugmentData=True):\n",
    "#    print(batch.shape, labels.shape)\n",
    "    #plotImagesFromBatch(batch)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Modificação do modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir o modelo base da squeezeNet \n",
    "squeezeNetModel = SqueezeNet(input_shape)\n",
    "\n",
    "# Escolher a camada que será o ponto de partida \n",
    "x = squeezeNetModel.get_layer(name=\"fire9/concat\").output\n",
    "\n",
    "#print([layer.name for layer in squeezeNetModel.layers])\n",
    "#print(\"\\n\\nFreeze layers up until \", squeezeNetModel.layers[-20].name)\n",
    "\n",
    "for layer in squeezeNetModel.layers:\n",
    "    layer.trainable = True#        layer.trainable = False\n",
    "\n",
    "x = Convolution2D(1024, (1, 1), padding='valid', name='conv10_new')(x)\n",
    "x = Activation('relu', name='relu_conv10_new')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Convolution2D(nbClasses, (1, 1), padding='valid', name='conv11_new')(x)\n",
    "x = Activation('relu', name='relu_conv11_new')(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Activation('softmax', name='loss_new')(x)\n",
    "\n",
    "\n",
    "# Não se esqueça de definir o nome modelo, onde baseSqueezeNetModel \n",
    "# é o modelo base da Squeeze que vc definiu ali em cima\n",
    "model = Model(squeezeNetModel.inputs, x, name='squeezenet_new_adam')\n",
    "model2 = Model(squeezeNetModel.inputs, x, name='squeezenet_new_adadelta')\n",
    "model3 = Model(squeezeNetModel.inputs, x, name='squeezenet_new_sgd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD, Adam\n",
    "#Compile o modelo\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.00001), metrics=['accuracy'])\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "model3.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True), metrics=['accuracy'])\n",
    "\n",
    "import keras.callbacks as callbacks\n",
    "\n",
    "tbCallBack = callbacks.TensorBoard(log_dir = \"./logs_squeeze8\")\n",
    "tbEarly = callbacks.EarlyStopping(monitor='val_acc',min_delta=0,patience=10,verbose=0, mode='auto')\n",
    "tbModelAdamChk = callbacks.ModelCheckpoint('.modeladam_weights.hdf5', save_best_only=True, monitor='val_acc', mode='max')\n",
    "tbModelAdadeltaChk = callbacks.ModelCheckpoint('.modeladadelta_weights.hdf5', save_best_only=True, monitor='val_acc', mode='max')\n",
    "tbModelSgdChk = callbacks.ModelCheckpoint('.modelsgd_weights.hdf5', save_best_only=True, monitor='val_acc', mode='max')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definir tamanho do batch e número de épocas\n",
    "batch_size = 8\n",
    "epochs = 30\n",
    "\n",
    "#Criação dos generators\n",
    "trainGenerator_adam = inf619utils.loadDatasetInBatches(train_files_adam, batch_size = batch_size, input_shape=input_shape, nbClasses=nbClasses,  shouldAugmentData=True)\n",
    "valGenerator_adam = inf619utils.loadDatasetInBatches(val_files_adam, batch_size = batch_size)\n",
    "\n",
    "#Fit nos dados\n",
    "hist = model.fit_generator(trainGenerator_adam, \n",
    "                    steps_per_epoch= int(trainSetSize / batch_size), \n",
    "                    epochs = epochs,\n",
    "                    validation_data = valGenerator_adam,  \n",
    "                    validation_steps = int(valSetSize / batch_size),\n",
    "                    callbacks=[tbCallBack, tbEarly, tbModelAdamChk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definir tamanho do batch e número de épocas\n",
    "batch_size = 8\n",
    "epochs = 30\n",
    "\n",
    "#Criação dos generators\n",
    "trainGenerator_adadelta = inf619utils.loadDatasetInBatches(train_files_adadelta, batch_size = batch_size, input_shape=input_shape, nbClasses=nbClasses, shouldAugmentData=True)\n",
    "valGenerator_adadelta = inf619utils.loadDatasetInBatches(val_files_adadelta, batch_size = batch_size)\n",
    "\n",
    "#Fit nos dados\n",
    "hist = model2.fit_generator(trainGenerator_adadelta, \n",
    "                    steps_per_epoch= int(trainSetSize / batch_size), \n",
    "                    epochs = epochs,\n",
    "                    validation_data = valGenerator_adadelta,  \n",
    "                    validation_steps = int(valSetSize / batch_size),\n",
    "                    callbacks=[tbCallBack, tbEarly, tbModelAdadeltaChk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "155/155 [==============================] - 8s 49ms/step - loss: 2.2775 - acc: 0.1460 - val_loss: 2.1626 - val_acc: 0.2763\n",
      "Epoch 2/30\n",
      "155/155 [==============================] - 9s 59ms/step - loss: 2.0322 - acc: 0.2847 - val_loss: 1.7374 - val_acc: 0.3987\n",
      "Epoch 3/30\n",
      "155/155 [==============================] - 16s 101ms/step - loss: 1.5950 - acc: 0.4653 - val_loss: 1.2459 - val_acc: 0.5681\n",
      "Epoch 4/30\n",
      "155/155 [==============================] - 17s 112ms/step - loss: 1.0955 - acc: 0.6169 - val_loss: 0.8502 - val_acc: 0.7542\n",
      "Epoch 5/30\n",
      "155/155 [==============================] - 17s 113ms/step - loss: 0.7617 - acc: 0.7379 - val_loss: 0.6854 - val_acc: 0.7508\n",
      "Epoch 6/30\n",
      "155/155 [==============================] - 17s 112ms/step - loss: 0.5794 - acc: 0.8064 - val_loss: 0.5693 - val_acc: 0.7973\n",
      "Epoch 7/30\n",
      "155/155 [==============================] - 17s 112ms/step - loss: 0.4640 - acc: 0.8379 - val_loss: 0.4861 - val_acc: 0.8306\n",
      "Epoch 8/30\n",
      "155/155 [==============================] - 17s 112ms/step - loss: 0.3710 - acc: 0.8742 - val_loss: 0.3909 - val_acc: 0.8837\n",
      "Epoch 9/30\n",
      "155/155 [==============================] - 17s 112ms/step - loss: 0.3241 - acc: 0.8927 - val_loss: 0.3898 - val_acc: 0.8439\n",
      "Epoch 10/30\n",
      "155/155 [==============================] - 17s 112ms/step - loss: 0.3069 - acc: 0.8928 - val_loss: 0.2085 - val_acc: 0.9435\n",
      "Epoch 11/30\n",
      "155/155 [==============================] - 17s 112ms/step - loss: 0.2329 - acc: 0.9226 - val_loss: 0.4685 - val_acc: 0.8804\n",
      "Epoch 12/30\n",
      "155/155 [==============================] - 17s 112ms/step - loss: 0.2271 - acc: 0.9347 - val_loss: 0.2249 - val_acc: 0.9369\n",
      "Epoch 13/30\n",
      "155/155 [==============================] - 17s 112ms/step - loss: 0.2359 - acc: 0.9185 - val_loss: 0.3887 - val_acc: 0.8804\n",
      "Epoch 14/30\n",
      "155/155 [==============================] - 19s 122ms/step - loss: 0.1938 - acc: 0.9274 - val_loss: 0.1678 - val_acc: 0.9668\n",
      "Epoch 15/30\n",
      "155/155 [==============================] - 17s 112ms/step - loss: 0.1849 - acc: 0.9444 - val_loss: 0.1985 - val_acc: 0.9203\n",
      "Epoch 16/30\n",
      "155/155 [==============================] - 17s 112ms/step - loss: 0.1722 - acc: 0.9468 - val_loss: 0.2603 - val_acc: 0.9136\n",
      "Epoch 17/30\n",
      "155/155 [==============================] - 13s 82ms/step - loss: 0.1512 - acc: 0.9403 - val_loss: 0.2572 - val_acc: 0.9302\n",
      "Epoch 18/30\n",
      "155/155 [==============================] - 14s 88ms/step - loss: 0.1609 - acc: 0.9492 - val_loss: 0.1875 - val_acc: 0.9468\n",
      "Epoch 19/30\n",
      "155/155 [==============================] - 18s 113ms/step - loss: 0.1467 - acc: 0.9581 - val_loss: 0.2001 - val_acc: 0.9468\n",
      "Epoch 20/30\n",
      "155/155 [==============================] - 18s 113ms/step - loss: 0.1394 - acc: 0.9556 - val_loss: 0.2010 - val_acc: 0.9468\n",
      "Epoch 21/30\n",
      "155/155 [==============================] - 18s 113ms/step - loss: 0.1310 - acc: 0.9581 - val_loss: 0.1076 - val_acc: 0.9502\n",
      "Epoch 22/30\n",
      "155/155 [==============================] - 18s 113ms/step - loss: 0.1070 - acc: 0.9694 - val_loss: 0.1284 - val_acc: 0.9767\n",
      "Epoch 23/30\n",
      "155/155 [==============================] - 17s 113ms/step - loss: 0.1164 - acc: 0.9645 - val_loss: 0.1573 - val_acc: 0.9468\n",
      "Epoch 24/30\n",
      "155/155 [==============================] - 18s 114ms/step - loss: 0.1107 - acc: 0.9669 - val_loss: 0.1337 - val_acc: 0.9601\n",
      "Epoch 25/30\n",
      "155/155 [==============================] - 17s 113ms/step - loss: 0.1218 - acc: 0.9589 - val_loss: 0.2110 - val_acc: 0.9435\n",
      "Epoch 26/30\n",
      "155/155 [==============================] - 17s 113ms/step - loss: 0.1061 - acc: 0.9694 - val_loss: 0.1071 - val_acc: 0.9601\n",
      "Epoch 27/30\n",
      "155/155 [==============================] - 17s 113ms/step - loss: 0.0794 - acc: 0.9798 - val_loss: 0.1588 - val_acc: 0.9568\n",
      "Epoch 28/30\n",
      "155/155 [==============================] - 18s 113ms/step - loss: 0.0934 - acc: 0.9645 - val_loss: 0.1044 - val_acc: 0.9635\n",
      "Epoch 29/30\n",
      "155/155 [==============================] - 18s 114ms/step - loss: 0.1065 - acc: 0.9718 - val_loss: 0.1846 - val_acc: 0.9369\n",
      "Epoch 30/30\n",
      "149/155 [===========================>..] - ETA: 0s - loss: 0.0658 - acc: 0.9773"
     ]
    }
   ],
   "source": [
    "#Definir tamanho do batch e número de épocas\n",
    "batch_size = 8\n",
    "epochs = 30\n",
    "\n",
    "#Criação dos generators\n",
    "trainGenerator_sgd = inf619utils.loadDatasetInBatches(train_files_sgd, batch_size = batch_size, input_shape=input_shape, nbClasses=nbClasses, shouldAugmentData=True)\n",
    "valGenerator_sgd = inf619utils.loadDatasetInBatches(val_files_sgd, batch_size = batch_size)\n",
    "\n",
    "#Fit nos dados\n",
    "hist = model3.fit_generator(trainGenerator_sgd, \n",
    "                    steps_per_epoch= int(trainSetSize / batch_size), \n",
    "                    epochs = epochs,\n",
    "                    validation_data = valGenerator_sgd,  \n",
    "                    validation_steps = int(valSetSize / batch_size),\n",
    "                    callbacks=[tbCallBack, tbEarly, tbModelSgdChk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicação do Modelo no Conjunto de Testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criação do generator p/ o conjunto de teste\n",
    "testGenerator_adam = inf619utils.loadDatasetInBatches(test_files_adam, batch_size=batch_size)\n",
    "\n",
    "# Load Best weights saved\n",
    "model.load_weights(filepath='.modeladam_weights.hdf5')\n",
    "\n",
    "#Teste\n",
    "metrics = model.evaluate_generator(testGenerator_adam, \n",
    "                                   steps=int(testSetSize/batch_size), \n",
    "                                   verbose=1)\n",
    "\n",
    "print(\"Test Loss Adam---> \", metrics[0])\n",
    "print(\"Test Accuracy Adam---> \", metrics[1])    #Test is balanced, so Acc is normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criação do generator p/ o conjunto de teste\n",
    "testGenerator_adadelta = inf619utils.loadDatasetInBatches(test_files_adadelta, batch_size=batch_size)\n",
    "\n",
    "# Load Best weights saved\n",
    "model2.load_weights(filepath='.modeladadelta_weights.hdf5')\n",
    "\n",
    "#Teste\n",
    "metrics = model2.evaluate_generator(testGenerator_adadelta, \n",
    "                                   steps=int(testSetSize/batch_size), \n",
    "                                   verbose=1)\n",
    "\n",
    "print(\"Test Loss Adadelta---> \", metrics[0])\n",
    "print(\"Test Accuracy Adadelta---> \", metrics[1])    #Test is balanced, so Acc is normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criação do generator p/ o conjunto de teste\n",
    "testGenerator_sgd = inf619utils.loadDatasetInBatches(test_files_sgd, batch_size=batch_size)\n",
    "\n",
    "# Load Best weights saved\n",
    "model3.load_weights(filepath='.modelsgd_weights.hdf5')\n",
    "\n",
    "#Teste\n",
    "metrics = model3.evaluate_generator(testGenerator_sgd, \n",
    "                                   steps=int(testSetSize/batch_size), \n",
    "                                   verbose=1)\n",
    "\n",
    "print(\"Test Loss SGD---> \", metrics[0])\n",
    "print(\"Test Accuracy SGD---> \", metrics[1])    #Test is balanced, so Acc is normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('squeezenet_adam_96.h5')\n",
    "model2.save('squeezenet_adadelta_984.h5')\n",
    "model3.save('squeezenet_sgd_986.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
